{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "329d476b-ed0c-4bcf-a695-bc6915ceaa05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"training.csv\",header=0)\n",
    "test = pd.read_csv(\"test.csv\",header=0)\n",
    "valid = pd.read_csv(\"validation.csv\",header=0)\n",
    "\n",
    "#type(train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "407f6377-bd94-4d17-872a-704206eb3fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens\n",
    "from nltk.tokenize import word_tokenize\n",
    "def token(content):\n",
    "    return word_tokenize(content,\"english\")\n",
    "\n",
    "#remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "final_words=[]\n",
    "def rm_stopwords(words):\n",
    "   final_words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "   final_words_string = ''.join(final_words)\n",
    "   return final_words_string\n",
    "\n",
    "# stemming + lemmatization\n",
    "from nltk.stem import SnowballStemmer \n",
    "snowball = SnowballStemmer(language='english')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def many_stem_lemma(list_of_words):\n",
    "    new_list =''\n",
    "    for word in list_of_words:\n",
    "      stem =  snowball.stem(word)\n",
    "      lemma = lemmatizer.lemmatize(stem,pos='v')\n",
    "      new_list += lemma\n",
    "    return new_list\n",
    "\n",
    "#combining all\n",
    "import re\n",
    "def transf(x):\n",
    "    lower_content = x.lower()\n",
    "    punct_content = re.sub(r'[^\\w\\s]', '', lower_content)\n",
    "\n",
    "    L = many_stem_lemma(rm_stopwords(token(punct_content)))\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23037112-f955-42ba-ba4b-7c44f0c0ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train['text'] = train['text'].apply(lambda x: transf(x))\n",
    "test['text'] = test['text'].apply(lambda x: transf(x))\n",
    "valid['text'] = valid['text'].apply(lambda x: transf(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8429d979-649e-4fcd-b65b-2d5007bc116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ee98e41-82b0-40ea-b52d-0fb1bb0193d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train['text']\n",
    "y_train = train['label']\n",
    "x_test = test['text']\n",
    "y_test = test['label']\n",
    "x_valid = valid['text']\n",
    "y_valid = valid['label']\n",
    "\n",
    "#x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57ca969d-8c53-481e-afbb-6393d3ec95b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "total_reviews=[]\n",
    "total_reviews.append(x_train.tolist())\n",
    "total_reviews.append(x_test.tolist())\n",
    "total_reviews.append(x_valid.tolist())\n",
    "\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(total_reviews)\n",
    "\n",
    "max_length = max([len(s) for s in total_reviews])\n",
    "vocab_size = len(tokenizer_obj.word_index) + 1\n",
    "\n",
    "x_train_tokens = tokenizer_obj.texts_to_sequences(x_train)\n",
    "x_test_tokens = tokenizer_obj.texts_to_sequences(x_test)\n",
    "x_valid_tokens = tokenizer_obj.texts_to_sequences(x_valid)\n",
    "\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_length, padding='post')\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_length, padding='post')\n",
    "x_valid_pad = pad_sequences(x_valid_tokens, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59e84a59-938f-4100-84eb-d0d0bdbd0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(x_train_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed3ddf4e-83e6-4527-b17b-09152bb5826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# First Model \n",
    "class RNN_model1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN_model1, self).__init__()\n",
    "        self.lstm1 =nn.LSTM(16,32,batch_first=True)\n",
    "        self.lstm2 =nn.LSTM(32,64,batch_first=True)\n",
    "        self.lstm3 =nn.LSTM(64,128,batch_first=True)\n",
    "        self.flat =nn.Flatten()\n",
    "        self.dense =nn.Linear(128, 6)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        outp1, _ = self.lstm1(x)\n",
    "        outp2, _ = self.lstm2(outp1)\n",
    "        outp3, _ = self.lstm3(outp2)\n",
    "        outp = self.flat(outp3.squeeze())\n",
    "        outp = self.dense(outp)\n",
    "        probs = F.softmax(outp, dim=1)  \n",
    "        predicted_class = torch.argmax(probs, dim=1)\n",
    "        return predicted_class.to(torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c1f5b82-4c58-44a8-8cfc-7502ef3e70e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_model1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df0dc82e-b46c-4269-b924-63a4cbfd6247",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funct = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f90b6549-a620-4000-a5bb-cd2a8d5976b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train_pad, dtype=torch.float32)\n",
    "#x_train_tensor = x_train_tensor.unsqueeze(0)\n",
    "#x_train_tensor = x_train_tensor.view(-1,16000)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "932093e6-0bad-42be-80b9-3183d12724aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(x_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c84368fa-6152-4761-bf61-a1336ca49e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bbb31997-0e51-4cff-9907-37d07056b550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for x, y in train_dataloader:\\n    print(y.shape)'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for x, y in train_dataloader:\n",
    "    print(y.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "649c0149-2425-4c50-9961-a7c9a938f067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1   loss:  36.05704524517059   accuracy:  0.29162500604521485\n",
      "epoch:  2   loss:  36.05704521894455   accuracy:  0.29162500612903386\n",
      "epoch:  3   loss:  36.05704517453909   accuracy:  0.2916250058123842\n",
      "epoch:  4   loss:  36.05704520523548   accuracy:  0.2916250059939921\n",
      "epoch:  5   loss:  36.057045257091524   accuracy:  0.29162500591482965\n",
      "epoch:  6   loss:  36.057045242488385   accuracy:  0.2916250058542937\n",
      "epoch:  7   loss:  36.0570452389121   accuracy:  0.2916250059893355\n",
      "epoch:  8   loss:  36.05704519212246   accuracy:  0.2916250059613958\n",
      "epoch:  9   loss:  36.057045176029206   accuracy:  0.2916250059893355\n",
      "epoch:  10   loss:  36.057045287787915   accuracy:  0.2916250057751313\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "epochs= 10\n",
    "for epoch in range(epochs):\n",
    "    # intitialisations\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    model.train()\n",
    "    ln = 0\n",
    "    for ids, (x,y) in enumerate(train_dataloader):\n",
    "        pred = model(x)\n",
    "        loss = loss_funct(pred,y)\n",
    "        acc = accuracy(pred, y, task=\"multiclass\", num_classes=6)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc.item()\n",
    "        ln +=1\n",
    "        \n",
    "    avg_loss = total_loss / ln \n",
    "    avg_acc = total_acc / ln \n",
    "    print(\"epoch: \",epoch+1,\"  loss: \",avg_loss,\"  accuracy: \",avg_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8fd391ae-309d-4e51-81bd-ba220dd9ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Second Model \n",
    "class RNN_model2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN_model2, self).__init__()\n",
    "        self.gru1 =nn.GRU(16000,32,batch_first=True)\n",
    "        self.gru2 =nn.GRU(32,64,batch_first=True)\n",
    "        self.gru3 =nn.GRU(64,128,batch_first=True)\n",
    "        self.flat =nn.Flatten()\n",
    "        self.dense =nn.Linear(128, 6)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        outp1, _ = self.gru1(x)\n",
    "        outp2, _ = self.gru2(outp1)\n",
    "        outp3, _ = self.gru3(outp2)\n",
    "        outp = self.flat(outp3.squeeze())\n",
    "        outp = self.dense(outp)\n",
    "        probs = F.softmax(outp, dim=1)  \n",
    "        predicted_class = torch.argmax(probs, dim=1)\n",
    "        return predicted_class.to(torch.float32)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3c25f747-4cb6-4056-8263-798ed7afc8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = RNN_model2()\n",
    "optimizer = torch.optim.SGD(model2.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "28f25b61-bd80-462b-94ef-09db51099897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1   loss:  36.057045247852805   accuracy:  0.035750000583939255\n",
      "epoch:  2   loss:  36.057045160532   accuracy:  0.03575000056065619\n",
      "epoch:  3   loss:  36.05704523533583   accuracy:  0.03575000055134296\n",
      "epoch:  4   loss:  36.05704524487257   accuracy:  0.035750000583939255\n",
      "epoch:  5   loss:  36.05704522818327   accuracy:  0.03575000057462603\n",
      "epoch:  6   loss:  36.05704524934292   accuracy:  0.0357500005653128\n",
      "epoch:  7   loss:  36.05704527974129   accuracy:  0.03575000057928264\n",
      "epoch:  8   loss:  36.05704522043467   accuracy:  0.0357500005653128\n",
      "epoch:  9   loss:  36.05704526782036   accuracy:  0.03575000055599958\n",
      "epoch:  10   loss:  36.057045251727104   accuracy:  0.03575000056065619\n"
     ]
    }
   ],
   "source": [
    "epochs= 10\n",
    "for epoch in range(epochs):\n",
    "    # intitialisations\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    model2.train()\n",
    "    ln = 0\n",
    "    for ids, (x,y) in enumerate(train_dataloader):\n",
    "        pred = model2(x)\n",
    "        loss = loss_funct(pred,y)\n",
    "        acc = accuracy(pred, y, task=\"multiclass\", num_classes=6)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc.item()\n",
    "        ln +=1\n",
    "        \n",
    "    avg_loss = total_loss / ln \n",
    "    avg_acc = total_acc / ln \n",
    "    print(\"epoch: \",epoch+1,\"  loss: \",avg_loss,\"  accuracy: \",avg_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3058c2-b9c2-49f0-b988-372fd67e5654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b65b2199-aaf7-406b-8286-66345bf61142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    3\n",
       "3    2\n",
       "4    3\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece14f3-91ff-4023-8bda-efd622eaa06a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
